https://morvanzhou.github.io/tutorials/machine-learning/tensorflow

1、安装numpy，里面包含各种线代矩阵以及随机数等算法
   # 使用 python 3+:
   pip3 install numpy
   # 使用 python 2+:
   pip install numpy

2、安装tensorflow，用于机器学习，目前只支持Python 3.5/3.6 (64bit) 版本
   # CPU 版的
   pip3 install --upgrade tensorflow
   # GPU 版的
   pip3 install --upgrade tensorflow-gpu
   
   注意：window安装前检查电脑是否有 Windows 的 Microsoft Visual C++ 2015 redistributable update 3 64 bit

3、安装matplotlib，用于绘图
   python -mpip install -U pip
   python -mpip install -U matplotlib

4、加速神经网络学习：
   1）SGD：最基础方法，把数据拆成小批小批，每次使用批数据训练，速度最慢
   2）Momentum: 
      m = b1 * m - Learning rate * dx
      w += m
      传统方式是w += -Learning rate * dx，这种是在权值累加上一个负的学习率乘以校正值
      这种会走很多弯路，比较慢，Momentum相当于加了个斜坡，依靠向下的惯性来让他走的弯路变少
   3）AdaGrad：
      v += dx^2
      w += -Learning rate * dx / √￣v
      AdaGrad相当于给他加了双不好走的鞋子，鞋子成为走弯路的阻力
   4）RMSProp:
      v = b1 * v + (1 - b1) * dx^2
      w += -Learning rate * dx / √￣v
      RMSProp合并了Momentum和AdaGrad两种，但是合并并不完全
   4）Adam:
      m = b1 * m + (1 - b1) * dx
      v = b2 * v + (1 - b2) * dx^2
      w += -Learning rate * m / √￣v
      Adam相当于Momentum和AdaGrad的结合，实验证明，大多数时候，使用adam都能又快又好的达到目标

5、优化器，tensorflow目前有七种优化器，分别是：
   GradientDescentOptimizer （SGD）
   AdagradOptimizer 
   AdagradDAOptimizer 
   MomentumOptimizer 
   AdamOptimizer 
   FtrlOptimizer 
   RMSPropOptimizer

6、模型展示：
    sess = tf.Session() # get session
    with tf.name_scope(layer_name) # 需要展示的节点需要用这个方法来定义展示节点名称，然后在框架图中就能看到这个节点了
    writer = tf.summary.FileWriter("logs/", sess.graph)
    利用graph来将定义的框架信息收集起来，然后放在/logs目录下
    命令行进入logs目录所在路径，执行命令"tensorboard --logdir logs"后会生成一个url，将这个url在浏览器上打开就能看到
    框架图了，框架图上双击可以展开详细图

7、记录权值、偏移值、输出的变化曲线：
    writer = tf.summary.FileWriter("logs/", sess.graph)  # tensorflow >=0.12
    merged = tf.summary.merge_all()  # tensorflow >= 0.12，把所有的 summaries 合并到一起
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b')  # 偏移，tf.zeros([n, m])代表生成一个 n x m 矩阵
    # tf.histogram_summary(layer_name+'/biase',biases)   # tensorflow 0.12 以下版的
    tf.summary.histogram('/value', biases)  # Tensorflow >= 0.12

    tf.summary就是记录这些值的变化过程，histogram代表放在HISTOGRAM标签下，需要记录的时候调用函数：    
    rs = sess.run(merged, feed_dict={xs: x_data, ys: y_data})
    writer.add_summary(rs, i) # 写入log

8、防止过拟合：
   有一种专门用在神经网络的正规化的方法, 叫作 dropout. 在训练的时候, 我们随机忽略掉一些神经元和神经联结 , 
   是这个神经网络变得”不完整”. 用一个不完整的神经网络训练一次，使用dropout方法如下：
    Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)  # y = wx + b
    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)

    keep_prob是保留率，就是每次结果保留的比例，keep_prob=1就没有dropout效果了，
    使用dropout的情景是训练数据比较少的情况下比较明显，训练数据多的话效果不明显

9、卷积神经网络：
   通过定义卷积层和池化来逐层提取特征，其中卷积层通常扫描步伐为1，就是每次移动距离都是1，扫描过程就是像
   拿手电筒从图片左上角开始遍历整张图片，遍历到的区域会生成一个特征值，特征值的计算是将卷积核中的每个值和扫描区域对应的值相乘
   然后将所有相乘结果求和得到一个新的值，因为移动距离为1，所以从左到右，从上到下移动过程中生成的特征层大小就和
   原始数据大小一样，然后经过池化来压缩卷积层数据，减少系统复杂度，最后定义一个分类器将卷积结果进行分类就能得到预估值了

   卷积神经网络中的一个特点就是权值共享，即每个滤波器在扫描图像的每个区域时，滤波器本身是不变的，
   假如有一张输入图像，先假定它是单通道的灰度图像，现在有一个[3 × 3]的卷积核，
   卷积核权重假如是[1，0，1；0，1，0；0，0，1]。我现在从左到右，从上到下，用这个卷积核在输入图像上滑动，每到一处，
   都是用同样的权重，对图像进行滤波，得到一个特征图。这就是所谓的权值

10、保存训练结果数据：
    每次运行结束后通过下面代码保存sess数据
    saver = tf.train.Saver()
    save_path = saver.save(sess, "my_net/save_net.ckpt")#保存训练结果

    下次运行时先调用下面方法来先加载上次数据，然后再开始训练
    saver.restore(sess, "my_net/save_net.ckpt")

11、循环神经网络RNN可以用来分类，它分析数据是建立在前面数据的基础上，比如分析 Data0 的时候, 我们把分析结果存入记忆.
    然后当分析 data1的时候, RNN会产生新的记忆, 但是新记忆和老记忆是没有联系的. 我们就简单的把老记忆调用过来, 一起分析.
    如果继续分析更多的有序数据 , RNN就会把之前的记忆都累积起来, 一起分析

    不过RNN有个弊端是会出现梯度消失或者梯度爆炸，在反向传递得到的误差的时候, 他在每一步都会乘以一个自己的参数 W.
    如果这个 W 是一个小于1 的数, 比如0.9. 这个0.9 不断乘以误差, 误差传到初始时间点也会是一个接近于零的数,
    所以对于初始时刻, 误差相当于就消失了. 我们把这个问题叫做梯度消失或者梯度弥散 Gradient vanishing.
    反之如果 W 是一个大于1 的数, 比如1.1 不断累乘, 则到最后变成了无穷大的数, RNN被这无穷大的数撑死了, 这种情况我们叫做剃度爆炸

    LSTM 就是为了解决这个问题而诞生的. LSTM 和普通 RNN 相比, 多出了三个控制器. (输入控制, 输出控制, 忘记控制). 现在, LSTM RNN 内部的情况是这样的：
    他多了一个控制全局的记忆, 为了方便理解, 我们把它想象成电影或游戏当中的主线剧情. 而原本的 RNN 体系就是分线剧情.
    三个控制器都是在原始的 RNN 体系上, 我们先看 输入方面 , 如果此时的分线剧情对于剧终结果十分重要, 输入控制就会将这个分线剧情按重要程度 写入主线剧情进行分析.
    再看忘记方面, 如果此时的分线剧情更改了我们对之前剧情的想法, 那么忘记控制就会将之前的某些主线剧情忘记, 按比例替换成现在的新剧情.
    所以 主线剧情的更新就取决于输入和忘记控制. 最后的输出方面, 输出控制会基于目前的主线剧情和分线剧情判断要输出的到底是什么.基于这些控制机制,
    LSTM 就像延缓记忆衰退的良药, 可以带来更好的结果

12、reshape分为两种，一种是ndArray.reshape，这个返回值还是一个ndArray，另一个是tensorflow.reshape，这个返回是一个Tensor对象，需要用sess.run()才能显示结果

13、_, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})，第一个参数是fetches，可以是一个元素或一个数组，运行时会执行fetches里面每个元素，然后
   每个元素都会产生一个对应结果，上面这个就是optimizer返回_，cost返回c

14、mnist = input_data.read_data_sets("/tmp/data/", one_hot=True) one_hot为True代表label(即每个xs对应的ys)的值用一个vector表示，这个向量总共一行十列，0是填充值，1
   是具体值所在位置，比如3就变成了[0,0,0,1,0,0,0,0,0,0],one_hot为False代表不用vector表示，3就是3

15、在神经网络中, 数据分布对训练会产生影响. 比如某个神经元 x 的值为1, 某个 Weights 的初始值为 0.1, 这样后一层神经元计算结果就是 Wx = 0.1;
    又或者 x = 20, 这样 Wx 的结果就为 2. 现在还不能看出什么问题, 但是, 当我们加上一层激励函数, 激活这个 Wx 值的时候, 问题就来了.
    如果使用 像 tanh 的激励函数, Wx 的激活值就变成了 ~0.1 和 ~1, 接近于 1 的数据已经处在了激励函数的饱和阶段, 也就是如果 x 无论再怎么扩大,
    tanh 激励函数输出值也还是接近1. 换句话说, 神经网络在初始阶段已经不对那些比较大的 x 特征范围 敏感了. 这样很糟糕,
    想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别, 这就证明我的感官系统失效了. 当然我们是可以用之前提到的对数据做 normalization 预处理,
    使得输入的 x 变化范围不会太大, 让输入值经过激励函数的敏感部分. 但刚刚这个不敏感问题不仅仅发生在神经网络的输入层, 而且在隐藏层中也经常会发生

    因此，可以使用tf.nn.batch_normalization(xs, mean, var, shift, scale, epsilon)来预处理数据

16、python中对象传递是把对象地址传递过去，因此改变对象内容会对原来对象产生相同影响，但是如果将该对象引用指向另一个地址，原来的对象是不会受影响的，
    比如在批标准化数据时，传入的Tensor对象会被重新指向另一个地址，xs = tf.nn.batch_normalization(inputs, mean, var, shift, scale, epsilon)，这样是
    不会影响原来的xs对象的

17、 Q learning是基于价值的一种机器学习算法，每次只选取价值最高的作为下一步，属于离线学习，
     它是通过维护一张Q表格，表格是关于 s（状态） 和 a（行为） 的一个存储，每个(s,a)对应一个值，每次判断会选取值最大的a作为下一个行为
     Q表格更新,假如有Q表格（s1,s2代表状态，a1,a2代表行为）

          a1    a2
     s1   -2    1
     s2   -4    2

     那么Q(s1,a2)现实值就等于 R + γ*Max[Q(s`, all actions)]，Q(s1,a2)预估值等于Q(s1,a2)，R代表眼前的奖励，γ代表对以往经验的重视程度，
     γ越大，就会越重视以往经验，属于回合更新，γ=0时只能看到眼前的奖励R，属于单步更新
     最终Q(s1,a2) = 老Q(s1,a2) + α*（现实值 - 估计值）


18、Sarsa也是基于价值的学习算法，同样拿上面的表格举例：
    Q(s1,a2)现实值就等于 R + γ*Q(s2，a2)，Q(s1,a2)预估值等于Q(s1,a2),γ=1时是回合更新，即把后续步骤都考虑进来再更新，γ=0时是单步更新，即与后面的步骤没有关系，
    最终Q(s1,a2) = 老Q(s1,a2) + α*（现实值 - 估计值）
    Sarsa与 Q learning不同的是它在 s2 这一步估算的动作也是接下来要做的动作，不像Q learning先观看一下在 s2 上选取哪一个动作会带来最大的奖励，
    但是在真正要做决定时, 却不一定会选取到那个带来最大奖励的动作

19、现实中Q表格状态非常多，所以无法全部存储，一种方式是利用神经网络，每次输入一个状态s，然后输出所有行为对应的值，再从中选取就行，
    神经网络每次更新的参数就是老的 NN(Neural Network) 参数 加学习率 alpha 乘以 Q 现实 和 Q 估计 的差距：w = 老w + α*（现实值 - 估计值）

20、 Deep Q Network 简称为 DQN，是Q-Learning与神经网络的结合，它主要由Experience replay 和 Fixed Q-targets组成，Experience replay用于学习之前的经历，
     就是先让智能体去探索环境，将经验（记忆）池累积到一定程度，在随机抽取出一批样本进行训练，
     使用fixed Q-targets, 我们就会在 DQN 中使用到两个结构相同但参数不同的神经网络, 预测 Q 估计的神经网络具备最新的参数,即 Q估计 = Q中参数是最新的，
     而预测 Q 现实的神经网络使用的参数则是很久以前的，即 Q现实 = R + γ*maxQ中参数是比较久之前的

     Deep Q Network训练前要先累积一定的状态和 Q 现实对应的数据，然后再把这些数据丢进神经网络去训练神经网络参数

     注意：
     1）reward设计要结合具体场景，比如游戏只是到达某一出口，那么reward可以设置成越靠近出口越大，如果是到达某一点，这点周围又有地雷这种东西，
        那么目标点的reward最大，地雷为负，靠近目标点的reward就不要越来越大了，而是都为0，不然会导致错误的引导
     2）如果状态太多的话一开始是没法全部预定义的，这时候就要靠实际训练过程中如果发现状态不存在，就把该状态添加进去来不断完善状态数组

21、算法选择上可以先考虑动作是否连续，连续动作 (动作是一个连续值, 比如旋转角度)
        Policy gradient
        DDPG
        A3C
        PPO
    离散动作 (动作是一个离散值, 比如向前,向后走)
        Q-learning
        DQN
        A3C
        PPO

22、如果数据不是浮点数的话就需要先转换成浮点数才能进行训练，如果数据种类不多的话可以用
    pandas.get_dummies(data, prefix=data.columns)把一个dataFrame转成one_hot结构，即每一行数据都用一组0和1的序列表示，这样便于用于神经网络

23、神经网络识别成语的话，因为权值都是浮点数，所以输入也是浮点数，因此必须先把输入的四个汉字先转成byte数组，然后再转成float数组作为输入，
    然后用词典中的成语进行训练

24、在tensorflow中训练神经网络时，训练到一半，出现Nan in summary histogram for:xxx，这种情况不一定每次都出现，有时候训练时会出现，有时候则不会。
    出现这种情况，一般是由以下三种情况造成
    1）Loss的计算采用了cross_entropy = -tf.reduce_sum(y_*tf.log(y))，如果y的值 = 0，则log(y)则会出错，
       解决方法是，将上式修改为：cross_entropy = -tf.reduce_sum(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))
    2）一般是由于优化器的学习率设置不当导致的，此时可以尝试使用更小的学习率进行训练来解决这样的问题
    3) 训练数据量太少

25、关于隐藏层大小的经验法则是在输入层和输出层之间，为了计算隐藏层大小我们使用一个一般法则：（输入大小+输出大小）*2/3
    当然也可以一个个试隐藏层大小，找到泛化最好的那个大小

26、要不要选择机器学习或者选择分类还是回归解决某一实际问题，个人的感悟是以下三点：
    1）要不要选择机器学习来解决这个问题，并不是所有问题都能用机器学习解决的，如果输入和输出并没有任何规律可言，那么机器学习也无能为力。
    2）选择分类，那么类别就是有限的，而且最重要的是输入数据要和类别之间有一定的规律，即输入的数据通过一定判断能够准确识别出所属类别，另外输入的数据必须覆盖所有
       类别，不然模型就不具有泛化能力，比较典型的像手写数字识别，因为输入的图片像素和对应的权值加权和能唯一确定这幅图像的形状，也就是类别，所以可以选择分类。
       但是像判断一个词是不是成语这种输入和类别没有明确关系的就不适合选择分类了。
    3）选择回归，这种输出就是单个值，即找出x和y的拟合曲线，如果你判断一个问题不适合分类，那么可以试试回归。


27、DQN 基于 Q-learning, Q-Learning 中有 Qmax, Qmax 会导致 Q现实当中的过估计 (overestimate). 而 Double DQN 就是用来解决过估计的. 在实际问题中,
    如果你输出你的 DQN 的 Q 值, 可能就会发现, Q 值都超级大. 这就是出现了 overestimate，因为我们的神经网络预测 Qmax 本来就有误差,
    每次也向着最大误差的 Q现实 改进神经网络, 就是因为这个 Qmax 导致了 overestimate. 所以 Double DQN 的想法就是引入另一个神经网络来打消一些最大误差的影响.
    而 DQN 中本来就有两个神经网络, 我们何不利用一下这个地理优势呢. 所以, 我们用 Q估计的神经网络估计 Q现实 中 Qmax(s', a') 的最大动作值.
    然后用这个被 Q估计 估计出来的动作来选择 Q现实 中的 Q(s')，因为Q估计的权值是最新的，所以用它来估计Q现实中的Qmax会比较准确

28、DQN学习过程中样本的选取是随机的，这样会导致学习的速度比较慢，所以需要优先级来选取样本，那么样本的优先级是怎么定的呢? 我们可以用到 TD-error,
    也就是 Q现实 - Q估计 来规定优先学习的程度. 如果 TD-error 越大, 就代表我们的预测精度还有很多上升空间, 那么这个样本就越需要被学习, 也就是优先级 p 越高

    有了 TD-error 就有了优先级 p, 那我们如何有效地根据 p 来抽样呢? 如果每次抽样都需要针对 p 对所有样本排序, 这将会是一件非常消耗计算能力的事.
    好在我们还有其他方法, 这种方法不会对得到的样本进行排序. 就是SumTree

    抽样时, 我们会将 p 的总合 除以 batch size, 分成 batch size 那么多区间, (n=sum(p)/batch_size). 如果将所有 node 的 priority 加起来是42的话,
    我们如果抽6个样本, 这时的区间拥有的 priority 可能是这样.
    [0-7], [7-14], [14-21], [21-28], [28-35], [35-42]

    然后在每个区间里随机选取一个数. 比如在第区间 [21-28] 里选到了24, 就按照这个 24 从优先级二叉树SumTree的顶点开始向下搜索.
    每次先和节点的左孩子节点比较，如果24比它大就选择右边并且减去左孩子节点的值，否则选择左边且不需要减去任何值，循环下去直到遇到叶子节点,
    然后根据选择的叶子节点的位置算出样本的位置，这样就获取到选择的样本数据以及优先级值，最后拿去训练就行了

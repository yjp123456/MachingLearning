https://morvanzhou.github.io/tutorials/machine-learning/tensorflow

1、安装numpy，里面包含各种线代矩阵以及随机数等算法
   # 使用 python 3+:
   pip3 install numpy
   # 使用 python 2+:
   pip install numpy

2、安装tensorflow，用于机器学习，目前只支持Python 3.5/3.6 (64bit) 版本
   # CPU 版的
   pip3 install --upgrade tensorflow
   # GPU 版的
   pip3 install --upgrade tensorflow-gpu
   
   注意：window安装前检查电脑是否有 Windows 的 Microsoft Visual C++ 2015 redistributable update 3 64 bit

3、安装matplotlib，用于绘图
   python -mpip install -U pip
   python -mpip install -U matplotlib

4、加速神经网络学习：
   1）SGD：最基础方法，把数据拆成小批小批，每次使用批数据训练，速度最慢
   2）Momentum: 
      m = b1 * m - Learning rate * dx
      w += m
      传统方式是w += -Learning rate * dx，这种是在权值累加上一个负的学习率乘以校正值
      这种会走很多弯路，比较慢，Momentum相当于加了个斜坡，依靠向下的惯性来让他走的弯路变少
   3）AdaGrad：
      v += dx^2
      w += -Learning rate * dx / √￣v
      AdaGrad相当于给他加了双不好走的鞋子，鞋子成为走弯路的阻力
   4）RMSProp:
      v = b1 * v + (1 - b1) * dx^2
      w += -Learning rate * dx / √￣v
      RMSProp合并了Momentum和AdaGrad两种，但是合并并不完全
   4）Adam:
      m = b1 * m + (1 - b1) * dx
      v = b2 * v + (1 - b2) * dx^2
      w += -Learning rate * m / √￣v
      Adam相当于Momentum和AdaGrad的结合，实验证明，大多数时候，使用adam都能又快又好的达到目标

5、优化器，tensorflow目前有七种优化器，分别是：
   GradientDescentOptimizer （SGD）
   AdagradOptimizer 
   AdagradDAOptimizer 
   MomentumOptimizer 
   AdamOptimizer 
   FtrlOptimizer 
   RMSPropOptimizer

6、模型展示：
    sess = tf.Session() # get session
    with tf.name_scope(layer_name) # 需要展示的节点需要用这个方法来定义展示节点名称，然后在框架图中就能看到这个节点了
    writer = tf.summary.FileWriter("logs/", sess.graph)
    利用graph来将定义的框架信息收集起来，然后放在/logs目录下
    命令行进入logs目录所在路径，执行命令"tensorboard --logdir logs"后会生成一个url，将这个url在浏览器上打开就能看到
    框架图了，框架图上双击可以展开详细图

7、记录权值、偏移值、输出的变化曲线：
    writer = tf.summary.FileWriter("logs/", sess.graph)  # tensorflow >=0.12
    merged = tf.summary.merge_all()  # tensorflow >= 0.12，把所有的 summaries 合并到一起
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b')  # 偏移，tf.zeros([n, m])代表生成一个 n x m 矩阵
    # tf.histogram_summary(layer_name+'/biase',biases)   # tensorflow 0.12 以下版的
    tf.summary.histogram('/value', biases)  # Tensorflow >= 0.12

    tf.summary就是记录这些值的变化过程，histogram代表放在HISTOGRAM标签下，需要记录的时候调用函数：    
    rs = sess.run(merged, feed_dict={xs: x_data, ys: y_data})
    writer.add_summary(rs, i) # 写入log

8、防止过拟合：
   有一种专门用在神经网络的正规化的方法, 叫作 dropout. 在训练的时候, 我们随机忽略掉一些神经元和神经联结 , 
   是这个神经网络变得”不完整”. 用一个不完整的神经网络训练一次，使用dropout方法如下：
    Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)  # y = wx + b
    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)

    keep_prob是保留率，就是每次结果保留的比例，keep_prob=1就没有dropout效果了，
    使用dropout的情景是训练数据比较少的情况下比较明显，训练数据多的话效果不明显

9、卷积神经网络：
   通过定义卷积层和池化来逐层提取特征，其中卷积层通常扫描步伐为1，就是每次移动距离都是1，扫描过程就是像
   拿手电筒从图片左上角开始遍历整张图片，遍历到的区域会生成一个特征值，特征值的计算是将卷积核中的每个值和扫描区域对应的值相乘
   然后将所有相乘结果求和得到一个新的值，因为移动距离为1，所以从左到右，从上到下移动过程中生成的特征层大小就和
   原始数据大小一样，然后经过池化来压缩卷积层数据，减少系统复杂度，最后定义一个分类器将卷积结果进行分类就能得到预估值了

   卷积神经网络中的一个特点就是权值共享，即每个滤波器在扫描图像的每个区域时，滤波器本身是不变的，
   假如有一张输入图像，先假定它是单通道的灰度图像，现在有一个[3 × 3]的卷积核，
   卷积核权重假如是[1，0，1；0，1，0；0，0，1]。我现在从左到右，从上到下，用这个卷积核在输入图像上滑动，每到一处，
   都是用同样的权重，对图像进行滤波，得到一个特征图。这就是所谓的权值

10、保存训练结果数据：
    每次运行结束后通过下面代码保存sess数据
    saver = tf.train.Saver()
    save_path = saver.save(sess, "my_net/save_net.ckpt")#保存训练结果

    下次运行时先调用下面方法来先加载上次数据，然后再开始训练
    saver.restore(sess, "my_net/save_net.ckpt")

11、循环神经网络RNN可以用来分类，它分析数据是建立在前面数据的基础上，比如分析 Data0 的时候, 我们把分析结果存入记忆.
    然后当分析 data1的时候, RNN会产生新的记忆, 但是新记忆和老记忆是没有联系的. 我们就简单的把老记忆调用过来, 一起分析.
    如果继续分析更多的有序数据 , RNN就会把之前的记忆都累积起来, 一起分析

    不过RNN有个弊端是会出现梯度消失或者梯度爆炸，在反向传递得到的误差的时候, 他在每一步都会乘以一个自己的参数 W.
    如果这个 W 是一个小于1 的数, 比如0.9. 这个0.9 不断乘以误差, 误差传到初始时间点也会是一个接近于零的数,
    所以对于初始时刻, 误差相当于就消失了. 我们把这个问题叫做梯度消失或者梯度弥散 Gradient vanishing.
    反之如果 W 是一个大于1 的数, 比如1.1 不断累乘, 则到最后变成了无穷大的数, RNN被这无穷大的数撑死了, 这种情况我们叫做剃度爆炸

    LSTM 就是为了解决这个问题而诞生的. LSTM 和普通 RNN 相比, 多出了三个控制器. (输入控制, 输出控制, 忘记控制). 现在, LSTM RNN 内部的情况是这样的：
    他多了一个控制全局的记忆, 为了方便理解, 我们把它想象成电影或游戏当中的主线剧情. 而原本的 RNN 体系就是分线剧情.
    三个控制器都是在原始的 RNN 体系上, 我们先看 输入方面 , 如果此时的分线剧情对于剧终结果十分重要, 输入控制就会将这个分线剧情按重要程度 写入主线剧情进行分析.
    再看忘记方面, 如果此时的分线剧情更改了我们对之前剧情的想法, 那么忘记控制就会将之前的某些主线剧情忘记, 按比例替换成现在的新剧情.
    所以 主线剧情的更新就取决于输入和忘记控制. 最后的输出方面, 输出控制会基于目前的主线剧情和分线剧情判断要输出的到底是什么.基于这些控制机制,
    LSTM 就像延缓记忆衰退的良药, 可以带来更好的结果

12、reshape分为两种，一种是ndArray.reshape，这个返回值还是一个ndArray，另一个是tensorflow.reshape，这个返回是一个Tensor对象，需要用sess.run()才能显示结果

13、_, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})，第一个参数是fetches，可以是一个元素或一个数组，运行时会执行fetches里面每个元素，然后
   每个元素都会产生一个对应结果，上面这个就是optimizer返回_，cost返回c

14、mnist = input_data.read_data_sets("/tmp/data/", one_hot=True) one_hot为True代表label(即每个xs对应的ys)的值用一个vector表示，这个向量总共一行十列，0是填充值，1
   是具体值所在位置，比如3就变成了[0,0,0,1,0,0,0,0,0,0],one_hot为False代表不用vector表示，3就是3

15、在神经网络中, 数据分布对训练会产生影响. 比如某个神经元 x 的值为1, 某个 Weights 的初始值为 0.1, 这样后一层神经元计算结果就是 Wx = 0.1;
    又或者 x = 20, 这样 Wx 的结果就为 2. 现在还不能看出什么问题, 但是, 当我们加上一层激励函数, 激活这个 Wx 值的时候, 问题就来了.
    如果使用 像 tanh 的激励函数, Wx 的激活值就变成了 ~0.1 和 ~1, 接近于 1 的数据已经处在了激励函数的饱和阶段, 也就是如果 x 无论再怎么扩大,
    tanh 激励函数输出值也还是接近1. 换句话说, 神经网络在初始阶段已经不对那些比较大的 x 特征范围 敏感了. 这样很糟糕,
    想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别, 这就证明我的感官系统失效了. 当然我们是可以用之前提到的对数据做 normalization 预处理,
    使得输入的 x 变化范围不会太大, 让输入值经过激励函数的敏感部分. 但刚刚这个不敏感问题不仅仅发生在神经网络的输入层, 而且在隐藏层中也经常会发生

    因此，可以使用tf.nn.batch_normalization(xs, mean, var, shift, scale, epsilon)来预处理数据

16、python中对象传递是把对象地址传递过去，因此改变对象内容会对原来对象产生相同影响，但是如果将该对象引用指向另一个地址，原来的对象是不会受影响的，
    比如在批标准化数据时，传入的Tensor对象会被重新指向另一个地址，xs = tf.nn.batch_normalization(inputs, mean, var, shift, scale, epsilon)，这样是
    不会影响原来的xs对象的

17、 Q learning是基于价值的一种机器学习算法，每次只选取价值最高的作为下一步，属于离线学习，
     它是通过维护一张Q表格，表格是关于 s（状态） 和 a（行为） 的一个存储，每个(s,a)对应一个值，每次判断会选取值最大的a作为下一个行为
     Q表格更新,假如有Q表格（s1,s2代表状态，a1,a2代表行为）

          a1    a2
     s1   -2    1
     s2   -4    2

     那么Q(s1,a2)现实值就等于 R + γ*Max[Q(s`, all actions)]，Q(s1,a2)预估值等于Q(s1,a2)，R代表现实中的奖励，γ代表深度，γ=1时能清清楚楚地看到之后所有步的全部价值，属于回合更新
     γ=0时只能看到眼前的奖励R，属于单步更新
     最终Q(s1,a2) = 老Q(s1,a2) + α*（现实值 - 估计值）
     Q learning代码样例：http://blog.csdn.net/gongxiaojiu/article/details/73345808

18、Sarsa也是基于价值的学习算法，同样拿上面的表格举例：
    Q(s1,a2)现实值就等于 R + γ*Q(s2，a2)，Q(s1,a2)预估值等于Q(s1,a2),γ=1时是回合更新，即把后续步骤都考虑进来再更新，γ=0时是单步更新，即与后面的步骤没有关系，
    最终Q(s1,a2) = 老Q(s1,a2) + α*（现实值 - 估计值）
    Sarsa与 Q learning不同的是它在 s2 这一步估算的动作也是接下来要做的动作，不像Q learning先观看一下在 s2 上选取哪一个动作会带来最大的奖励，
    但是在真正要做决定时, 却不一定会选取到那个带来最大奖励的动作

19、现实中Q表格状态非常多，所以无法全部存储，一种方式是利用神经网络，每次输入一个状态s，然后输出所有行为对应的值，再从中选取就行，
    神经网络每次更新的参数就是老的 NN(Neural Network) 参数 加学习率 alpha 乘以 Q 现实 和 Q 估计 的差距：w = 老w + α*（现实值 - 估计值）

20、 Deep Q Network 简称为 DQN，是Q-Learning与神经网络的结合，它主要由Experience replay 和 Fixed Q-targets组成，Experience replay用于学习之前的经历，
     就是先让智能体去探索环境，将经验（记忆）池累积到一定程度，在随机抽取出一批样本进行训练，
     使用fixed Q-targets, 我们就会在 DQN 中使用到两个结构相同但参数不同的神经网络, 预测 Q 估计的神经网络具备最新的参数,即 Q估计 = Q中参数是最新的，
     而预测 Q 现实的神经网络使用的参数则是很久以前的，即 Q现实 = R + γ*maxQ中参数是比较久之前的

     Deep Q Network训练前要先累积一定的状态和 Q 现实对应的数据，然后再把这些数据丢进神经网络去训练神经网络参数

21、算法选择上可以先考虑动作是否连续，连续动作 (动作是一个连续值, 比如旋转角度)
        Policy gradient
        DDPG
        A3C
        PPO
    离散动作 (动作是一个离散值, 比如向前,向后走)
        Q-learning
        DQN
        A3C
        PPO

22、如果数据不是浮点数的话就需要先转换成浮点数才能进行训练，如果数据种类不多的话可以用
    pandas.get_dummies(data, prefix=data.columns)把一个dataFrame转成one_hot结构，即每一行数据都用一组0和1的序列表示，这样便于用于神经网络

23、神经网络识别成语的话，因为权值都是浮点数，所以输入也是浮点数，因此必须先把输入的四个汉字先转成byte数组，然后再转成float数组作为输入，
    然后用词典中的成语进行训练